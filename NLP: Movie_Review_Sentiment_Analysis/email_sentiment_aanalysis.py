# -*- coding: utf-8 -*-
"""Email_Sentiment_Aanalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pjkjQ_cPORQYfJEG1fvVMJQiBiai2Lhu

# MOVIE REVIEW SENTIMENT ANALYSIS (NLP Project)

The main aim of this project is to identify the sentiment of the review for a movie. The data is downloaded from KAGGLE.

The link to download the data:
https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount("/content/gdrive")

df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/IMDB Dataset.csv')

df['review'][1]

df['sentiment'].value_counts() #we can see that the dataset is balanced

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

"""# Basic Preprocessing"""

# Remove tags
# Remove URLS
# lowercase
# Revomve emojis
# Remove stopwords

# Remove tags

import re
def remove_tags(raw_text):
    cleaned_text = re.sub(re.compile('<.*?>'), '', raw_text)
    return cleaned_text

df['review'] = df['review'].apply(remove_tags)

df

#Remove URLS 

import re
def remove_urls (vTEXT):
    vTEXT = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', vTEXT, flags=re.MULTILINE)
    return(vTEXT)

df['review'] = df['review'].apply(remove_urls)

#LOWER CASING

df['review'] = df['review'].apply(lambda x:x.lower())

#STOP WORDS REMOVAL

#from nltk.corpus import stopwords
#sw_list = stopwords.words('english')

#f['review'] = df['review'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:" ".join(x))

df

#EMOJI REMOVAL

import re
def deEmojify(text):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

df['review'] = df['review'].apply(deEmojify)

#Tokenization

#from nltk.tokenize import word_tokenize

#df['review']= df['review'].apply(lambda text: word_tokenize(text))
#df.head()

#Stemming
#from nltk.stem.porter import PorterStemmer
#stemmer = PorterStemmer()

#def stem_words(text):
#    stem_text = [stemmer.stem(word) for word in text]
#    return stem_text

#df['review'] = df['review'].apply(lambda text: stem_words(text))
#df.head()

X = df.iloc[:,0:1]
y = df['sentiment']

X

y

#After Labelling the data

import pickle
import xgboost
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score,confusion_matrix

gnb = GaussianNB()
cv = CountVectorizer()
xgb = XGBClassifier()
encoder = LabelEncoder()
tfidf = TfidfVectorizer()
rf = RandomForestClassifier()


y = encoder.fit_transform(y)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)

def Bow(train_reviews,test_reviews):
    cv = CountVectorizer(max_features=1000)
    X_train_bow = cv.fit_transform(train_reviews).toarray()
    X_test_bow = cv.transform(test_reviews).toarray()
    
    return X_train_bow,X_test_bow

def bigram(train_reviews,test_reviews):
    cv = CountVectorizer(ngram_range=(1,2),max_features=1000)
    X_train_bigram = cv.fit_transform(train_reviews).toarray()
    X_test_bigram = cv.transform(test_reviews).toarray()
    
    return X_train_bigram,X_test_bigram

def ngram(train_reviews,test_reviews,n):
    cv = CountVectorizer(ngram_range=(1,n),max_features=1000)
    X_train_ngram = cv.fit_transform(train_reviews).toarray()
    X_test_ngram = cv.transform(test_reviews).toarray()
    
    return X_train_ngram,X_test_ngram

#TFIDF
def tf(train_reviews,test_reviews):
    tfidf = TfidfVectorizer(max_features=1000)
    X_train_tfidf = tfidf.fit_transform(train_reviews).toarray()
    X_test_tfidf = tfidf.transform(test_reviews)
    
    return X_train_tfidf,X_test_tfidf

#Models

def RandomForest(X_train_bow,X_test_bow,y_train):
    rf = RandomForestClassifier()
    rf.fit(X_train_bow,y_train)
    #with open('rf.pickle', 'wb') as f:
     #pickle.dump(rf, f)
    y_pred = rf.predict(X_test_bow)
    accuracy=accuracy_score(y_test,y_pred)

    return accuracy

def Gaussian(X_train_gaus,X_test_gaus,y_train):
    gnb = GaussianNB()
    gnb.fit(X_train_gaus,y_train)
    #with open('nb.pickle', 'wb') as f:
     #pickle.dump(rf, f)
    y_pred = gnb.predict(X_test_gaus)
    accuracy=accuracy_score(y_test,y_pred)
    
    return accuracy

def SVM(X_train_bow,X_test_bow,y_train):
    from sklearn import svm
    clf = svm.SVC(C=config['SVM']['C'])
    clf.fit(X_train_bow,y_train)
    #with open('svm.pickle', 'wb') as f:
     #pickle.dump(rf, f)
    y_pred = clf.predict(X_test_bow)
    accuracy=accuracy_score(y_test,y_pred)
    
    return accuracy

def Xgboost(X_train_gaus,X_test_gaus,y_train):
    xgb = XGBClassifier()
    xgb.fit(X_train_gaus,y_train)
    #with open('xgb.pickle', 'wb') as f:
     #pickle.dump(rf, f)
    y_pred = xgb.predict(X_test_gaus)
    accuracy=accuracy_score(y_test,y_pred)
    
    return accuracy

"""Random Forest"""

X_train_bow,X_test_bow=Bow(X_train['review'],X_test['review'])
accuracy=RandomForest(X_train_bow,X_test_bow,y_train)
print(accuracy)

#pickled_model = pickle.load(open('model_rf_bow.pkl', 'rb'))
#y_pred = pickled_model.predict(X_test_bow)
#accuracy=accuracy_score(y_test,y_pred)
#print(accuracy)

X_train_bigram,X_test_bigram=bigram(X_train['review'],X_test['review'])
accuracy=RandomForest(X_train_bigram,X_test_bigram,y_train)
print(accuracy)

n=3
X_train_ngram,X_test_ngram=ngram(X_train['review'],X_test['review'],n)
accuracy=RandomForest(X_train_ngram,X_test_ngram,y_train)
print(accuracy)

X_train_tfidf,X_test_tfidf=tf(X_train['review'],X_test['review'])
accuracy=RandomForest(X_train_tfidf,X_test_tfidf,y_train)
print(accuracy)

"""Naive Bayes"""

X_train_bow,X_test_bow=Bow(X_train['review'],X_test['review'])
accuracy=Gaussian(X_train_bow,X_test_bow,y_train)
print(accuracy)

"""SVM"""

X_train_bow,X_test_bow=Bow(X_train['review'],X_test['review'])
accuracy=SVM(X_train_bow,X_test_bow,y_train)
print(accuracy)

"""Xgboost"""

X_train_bow,X_test_bow=Bow(X_train['review'],X_test['review'])
accuracy=Xgboost(X_train_bow,X_test_bow,y_train)
print(accuracy)

X_train_tfidf,X_test_tfidf=tf(X_train['review'],X_test['review'])
accuracy=Xgboost(X_train_tfidf,X_test_tfidf,y_train)
print(accuracy)

"""#### Random forest and XGBoost algorithms with TFIDF as a vectorizer performed better when compared to other alogrithms"""

#Now Saving the model with pickle

with open('clf.pickle', 'wb') as f:
     pickle.dump(clf, f)

# We can load the saved model with pickle
        
with open('clf.pickle', 'rb') as f:
     clf = pickle.load(f)